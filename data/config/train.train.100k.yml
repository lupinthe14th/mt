tok_tgt_bpe_EOT_marker = </w>
bridge = copy
train_from = 
tok_tgt_mode = aggressive
tok_tgt_joiner = ￭
dropout_input = false
dropout_words = 0
model = 
curriculum = 0
save_model = /data/kyoto-100k-model
tok_src_segment_alphabet_change = false
rnn_size = 500
tgt_seq_length = 50
brnn_merge = sum
dropout = 0.3
layers = 2
h = false
scheduled_sampling = 1
rnn_type = LSTM
sample_perplexity_init = 15
end_epoch = 13
tgt_suffix = .tgt
lm_model = 
exp_host = 127.0.0.1
model_type = seq2seq
placeholder_constraints = false
tok_tgt_segment_numbers = false
scheduled_sampling_decay_rate = 0
start_decay_at = 9
min_learning_rate = 0
sample_tgt_vocab = false
tok_src_joiner_annotate = true
valid_src = 
tok_src_bpe_model = 
data = /data/kyoto-train.100k-train.t7
disable_logs = false
pdbrnn_merge = concat
use_pos_emb = true
coverage_norm = 0
max_tokens = 1800
sample_vocab = false
exp = 
max_num_unks = inf
start_epoch = 1
length_norm = 0
optim = sgd
gpuid = 0
sample_perplexity_max = -1.5
tok_tgt_segment_alphabet_change = false
tok_src_bpe_mode = suffix
encoder_type = rnn
report_every = 50
log_file = /data/log/train.train.100k.log
learning_rate = 1
tok_src_bpe_EOT_marker = </w>
global_attention = general
check_plength = false
input_feed = true
tok_tgt_bpe_BOT_marker = <w>
lexical_constraints = false
cnn_size = 500
tok_tgt_segment_case = false
max_pos = 50
tok_tgt_bpe_case_insensitive = false
tok_tgt_joiner_new = false
time_shift_feature = true
pdbrnn = false
fix_word_vecs_enc = false
train_src = 
dbrnn = false
log_level = INFO
dropout_type = naive
tgt_vocab = 
src_vocab_size = 50000
save_every = 5000
pdbrnn_reduction = 2
attention = global
src_vocab = 
report_progress_every = 100000
feat_merge = concat
seed = 3435
features_vocabs_prefix = 
brnn = false
tok_tgt_sentencepiece = 
tok_src_segment_numbers = false
scheduled_sampling_decay_type = linear
train_tgt = 
n_best = 1
gsample = 0
tok_src_joiner_new = false
cnn_layers = 2
disable_mem_optimization = false
uneven_batches = false
pre_word_vecs_enc = 
md = false
exp_port = 8889
save_config = 
dump_input_encoding = false
idx_files = false
tgt_words_min_frequency = 0
async_parallel_minbatch = 1000
tok_src_mode = aggressive
max_batch_size = 160
cnn_kernel = 3
sample_type = uniform
log_tag = 
tok_src_bpe_case_insensitive = false
feat_vec_size = 20
tok_tgt_joiner_annotate = true
pre_filter_factor = 1
tok_tgt_bpe_mode = suffix
decay_method = default
scheduled_sampling_scope = token
save_beam_to = 
hook_file = hooks/sentencepiece
train_dir = 
tgt_word_vec_size = 500
replace_unk_tagged = false
keep_frequency = false
valid_tgt = 
decay = default
replace_unk = false
tok_src_case_feature = false
word_vec_size = 0
param_init = 0.1
src_words_min_frequency = 0
continue = false
enc_layers = 0
tok_src_joiner = ￭
profiler = false
save_every_epochs = 1
sample = 0
tgt_vocab_size = 50000
max_grad_norm = 5
update_vocab = none
tok_src_bpe_BOT_marker = <w>
start_decay_score_delta = 0
src_seq_length = 50
lm_weight = 0.1
feat_vec_exponent = 0.7
tok_tgt_case_feature = false
phrase_table = 
preprocess_pthreads = 4
beam_size = 5
pre_word_vecs_dec = 
src_suffix = .src
async_parallel = false
start_iteration = 1
learning_rate_decay = 0.7
config = 
residual = false
tok_src_segment_case = false
validation_metric = perplexity
no_nccl = false
save_validation_translation_every = 0
tok_tgt_bpe_model = 
target_subdict = 
max_sent_length = 250
tok_tgt_segment_alphabet = 
eos_norm = 0
sort = true
dec_layers = 0
src_word_vec_size = 500
fp16 = false
limit_lexical_constraints = false
tok_src_segment_alphabet = 
fallback_to_cpu = false
tok_src_sentencepiece = 
shuffle = true
gsample_dist = 
fix_word_vecs_dec = false
