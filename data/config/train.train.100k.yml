tok_tgt_bpe_EOT_marker = </w>
bridge = copy
train_from = 
tok_tgt_mode = space
tok_tgt_joiner = ￭
dropout_input = false
dropout_words = 0
model = 
curriculum = 0
save_model = /data/kyoto-100k-model
tok_src_segment_alphabet_change = false
rnn_size = 500
tgt_seq_length = 50
beam_size = 5
dropout = 0.3
layers = 2
h = false
scheduled_sampling = 1
rnn_type = LSTM
sample_perplexity_init = 15
end_epoch = 13
tgt_suffix = .tgt
lm_model = 
exp_host = 127.0.0.1
model_type = seq2seq
sample = 0
tok_tgt_segment_numbers = false
scheduled_sampling_decay_rate = 0
start_decay_at = 9
min_learning_rate = 0
lm_weight = 0.1
tok_src_joiner_annotate = false
valid_src = 
tok_src_bpe_model = 
data = /data/kyoto-train.100k-train.t7
disable_logs = false
pdbrnn_merge = concat
use_pos_emb = true
coverage_norm = 0
max_tokens = 1800
sample_vocab = false
exp = 
max_num_unks = inf
start_epoch = 1
length_norm = 0
optim = sgd
gpuid = 0
sample_perplexity_max = -1.5
tok_tgt_segment_alphabet_change = false
tok_src_bpe_mode = suffix
encoder_type = rnn
no_nccl = false
log_file = /data/log/train.train.100k.log
learning_rate = 1
tok_src_bpe_EOT_marker = </w>
global_attention = general
check_plength = false
input_feed = true
tok_tgt_bpe_BOT_marker = <w>
lexical_constraints = false
cnn_size = 500
tok_tgt_segment_case = false
max_pos = 50
tok_tgt_bpe_case_insensitive = false
tok_tgt_joiner_new = false
time_shift_feature = true
train_src = 
pdbrnn = false
fix_word_vecs_enc = false
dbrnn = false
tgt_vocab = 
log_level = INFO
dropout_type = naive
placeholder_constraints = false
async_parallel_minbatch = 1000
src_vocab = 
report_progress_every = 100000
feat_merge = concat
seed = 3435
features_vocabs_prefix = 
pdbrnn_reduction = 2
attention = global
brnn = false
pre_word_vecs_enc = 
tok_src_segment_numbers = false
n_best = 1
gsample = 0
tok_src_joiner_new = false
scheduled_sampling_decay_type = linear
train_tgt = 
cnn_layers = 2
save_every = 5000
tok_src_case_feature = false
exp_port = 8889
save_config = 
src_vocab_size = 50000
idx_files = false
tgt_words_min_frequency = 0
md = false
tok_src_mode = space
max_batch_size = 160
cnn_kernel = 3
sample_type = uniform
log_tag = 
tok_src_bpe_case_insensitive = false
async_parallel = false
gsample_dist = 
pre_filter_factor = 1
tok_tgt_joiner_annotate = false
decay_method = default
feat_vec_exponent = 0.7
scheduled_sampling_scope = token
save_beam_to = 
train_dir = 
hook_file = 
replace_unk_tagged = false
keep_frequency = false
valid_tgt = 
decay = default
replace_unk = false
src_words_min_frequency = 0
word_vec_size = 0
param_init = 0.1
feat_vec_size = 20
continue = false
disable_mem_optimization = false
enc_layers = 0
tok_src_joiner = ￭
profiler = false
save_every_epochs = 1
sample_tgt_vocab = false
tgt_vocab_size = 50000
fix_word_vecs_dec = false
start_decay_score_delta = 0
tok_src_bpe_BOT_marker = <w>
max_grad_norm = 5
src_seq_length = 50
report_every = 50
tgt_word_vec_size = 500
tok_tgt_case_feature = false
phrase_table = 
preprocess_pthreads = 4
dump_input_encoding = false
pre_word_vecs_dec = 
src_suffix = .src
start_iteration = 1
learning_rate_decay = 0.7
config = 
residual = false
tok_src_segment_case = false
validation_metric = perplexity
update_vocab = none
save_validation_translation_every = 0
tok_tgt_bpe_model = 
target_subdict = 
max_sent_length = 250
tok_tgt_segment_alphabet = 
eos_norm = 0
sort = true
dec_layers = 0
src_word_vec_size = 500
fp16 = false
limit_lexical_constraints = false
tok_src_segment_alphabet = 
fallback_to_cpu = false
tok_tgt_bpe_mode = suffix
shuffle = true
uneven_batches = false
brnn_merge = sum
